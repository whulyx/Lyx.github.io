<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  

  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="&amp;emsp;&amp;emsp;本次学习的源码来自hzy46/Char-RNN-TensorFlow，利用LSTM神经网络来实验文本的生成，如诗歌、小说和代码等。 &amp;emsp;&amp;emsp;因为之前并没有写过神经网络的代码，只是对一些模型有基础的了解，所以本文仅起到记录作用，并不保证内容的正确性，之后也会根据知识的积累来对本文的纰漏之处进行修正。 model.py&amp;emsp;&amp;emsp;首先是CharRNN">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/2018/03/05/LSTM-Learning-CharRNN/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="&amp;emsp;&amp;emsp;本次学习的源码来自hzy46/Char-RNN-TensorFlow，利用LSTM神经网络来实验文本的生成，如诗歌、小说和代码等。 &amp;emsp;&amp;emsp;因为之前并没有写过神经网络的代码，只是对一些模型有基础的了解，所以本文仅起到记录作用，并不保证内容的正确性，之后也会根据知识的积累来对本文的纰漏之处进行修正。 model.py&amp;emsp;&amp;emsp;首先是CharRNN">
<meta property="og:updated_time" content="2018-03-05T03:20:50.187Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="&amp;emsp;&amp;emsp;本次学习的源码来自hzy46/Char-RNN-TensorFlow，利用LSTM神经网络来实验文本的生成，如诗歌、小说和代码等。 &amp;emsp;&amp;emsp;因为之前并没有写过神经网络的代码，只是对一些模型有基础的了解，所以本文仅起到记录作用，并不保证内容的正确性，之后也会根据知识的积累来对本文的纰漏之处进行修正。 model.py&amp;emsp;&amp;emsp;首先是CharRNN">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Suche"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-LSTM-Learning-CharRNN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2018/03/05/LSTM-Learning-CharRNN/" class="article-date">
  <time datetime="2018-03-05T03:27:47.715Z" itemprop="datePublished">2018-03-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>&emsp;&emsp;本次学习的源码来自<a href="https://github.com/hzy46/Char-RNN-TensorFlow.git" target="_blank" rel="noopener">hzy46/Char-RNN-TensorFlow</a>，利用LSTM神经网络来实验文本的生成，如诗歌、小说和代码等。</p>
<p>&emsp;&emsp;因为之前并没有写过神经网络的代码，只是对一些模型有基础的了解，所以本文仅起到记录作用，并不保证内容的正确性，之后也会根据知识的积累来对本文的纰漏之处进行修正。</p>
<h2 id="model-py"><a href="#model-py" class="headerlink" title="model.py"></a>model.py</h2><p>&emsp;&emsp;首先是CharRNN模型初始化需要的参数：<br>| 参数名 | 含义 |<br>| :-: | :-: |<br>| num_classes | 类的数量？姑且理解为文本分类的数量 |<br>| num_seqs | 一个batch中文本序列的数量 |<br>| num_steps | 一个文本序列的长度 |<br>| lstm_size | lstm模型中hidden state的数量 |<br>| num_layers | 隐藏层数 |<br>| learning_rate | 学习率，梯度下降中使用？ |<br>| grad_clip | 用于控制梯度膨胀，如果梯度向量的L2模超过grad_clip，则等比例缩小 |<br>| sampling | 用于控制train/sample的bool值 |<br>| train_keep_prob | 用于dropout，每批数据输入时神经网络中的每个单元会以1-keep_prob的概率不工作，可以防止过拟合(同layer间不进行dropout) |<br>| use_embedding | 是否添加embedding层，用于one_hot维度缩减？ |<br>| embedding_size | embedding层的列数大小 |</p>
<p>&emsp;&emsp;在<strong>init</strong>函数的最后，分别调用了：</p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">self.build_inputs()</span><br><span class="line">self.build_lstm()</span><br><span class="line">self.build_loss()</span><br><span class="line">self.build_optimizer()</span><br><span class="line">self.saver = tf.train.Saver()</span><br></pre></td></tr></table></figure>
</code></pre><p>&emsp;&emsp;<strong>build_inputs()</strong></p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def build_inputs(self):</span><br><span class="line">    with tf.name_scope(&apos;inputs&apos;):</span><br><span class="line">        self.inputs = tf.placeholder(tf.int32, shape=(</span><br><span class="line">            self.num_seqs, self.num_steps), name=&apos;inputs&apos;)</span><br><span class="line">        self.targets = tf.placeholder(tf.int32, shape=(</span><br><span class="line">            self.num_seqs, self.num_steps), name=&apos;targets&apos;)</span><br><span class="line">        self.keep_prob = tf.placeholder(tf.float32, name=&apos;keep_prob&apos;)</span><br><span class="line"></span><br><span class="line">        # 对于中文，需要使用embedding层</span><br><span class="line">        # 英文字母没有必要用embedding层</span><br><span class="line">        if self.use_embedding is False:</span><br><span class="line">            self.lstm_inputs = tf.one_hot(self.inputs, self.num_classes)</span><br><span class="line">        else:</span><br><span class="line">            with tf.device(&quot;/cpu:0&quot;):</span><br><span class="line">                embedding = tf.get_variable(&apos;embedding&apos;, [self.num_classes, self.embedding_size])</span><br><span class="line">                self.lstm_inputs = tf.nn.embedding_lookup(embedding, self.inputs)</span><br></pre></td></tr></table></figure>
</code></pre><p>&emsp;&emsp;设置输入层和预期输出层的placeholder，形状都是num_seqs * num_steps，构建embedding层的部分先不管，因为不是我们关心的部分。</p>
<p>&emsp;&emsp;<strong>build_lstm()</strong></p>
<pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">def build_lstm(self):</span><br><span class="line">    # 创建单个cell并堆叠多层</span><br><span class="line">    def get_a_cell(lstm_size, keep_prob):</span><br><span class="line">        lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)</span><br><span class="line">        drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)</span><br><span class="line">        return drop</span><br><span class="line"></span><br><span class="line">    with tf.name_scope(&apos;lstm&apos;):</span><br><span class="line">        cell = tf.nn.rnn_cell.MultiRNNCell(</span><br><span class="line">            [get_a_cell(self.lstm_size, self.keep_prob) for _ in range(self.num_layers)]</span><br><span class="line">        )</span><br><span class="line">        self.initial_state = cell.zero_state(self.num_seqs, tf.float32)</span><br><span class="line"></span><br><span class="line">        # 通过dynamic_rnn对cell展开时间维度</span><br><span class="line">        self.lstm_outputs, self.final_state = tf.nn.dynamic_rnn(cell, self.lstm_inputs, initial_state=self.initial_state)</span><br><span class="line"></span><br><span class="line">        # 通过lstm_outputs得到概率</span><br><span class="line">        seq_output = tf.concat(self.lstm_outputs, 1)</span><br><span class="line">        x = tf.reshape(seq_output, [-1, self.lstm_size])</span><br><span class="line"></span><br><span class="line">        with tf.variable_scope(&apos;softmax&apos;):</span><br><span class="line">            softmax_w = tf.Variable(tf.truncated_normal([self.lstm_size, self.num_classes], stddev=0.1))</span><br><span class="line">            softmax_b = tf.Variable(tf.zeros(self.num_classes))</span><br><span class="line"></span><br><span class="line">        self.logits = tf.matmul(x, softmax_w) + softmax_b</span><br><span class="line">        self.proba_prediction = tf.nn.softmax(self.logits, name=&apos;predictions&apos;)</span><br></pre></td></tr></table></figure>
</code></pre><p>&emsp;&emsp;首先是定义了get_a_cell(lstm_size, keep_prob)函数，用来生成单层的lstm cell，然后通过tf.nn.rnn_cell.MultiRNNCell()根据设置的layer数量生成多层LSTMCell，前一层的输出作为后一层的输入。之后通过zero_state()函数初始化state。</p>
<p>&emsp;&emsp;关于tf.nn.dynamic_rnn()可以参考<a href="http://blog.csdn.net/u010223750/article/details/71079036" target="_blank" rel="noopener">博客</a>，可以暂时用官方文档中的描述去理解：“Creates a recurrent neural network specified by RNNCell cell.”，返回outputs和state，它们的默认shape分别为[batch_size, max_time, cell.output_size]和[batch_size, cell.state_size]。</p>
<p>&emsp;&emsp;关于max_time是什么意思：从官方文档来看，默认情况下dynamic_rnn的input’shape是[batch_size, max_time, …]，所以可以推断出这里max_time应该是等同于num_steps。</p>
<p>&emsp;&emsp;接下来是对seq_output的reshape操作，从原来的[batch_size, max_time, cell.output_size]变成[1, lstm_size]的一维向量。因为之前对tf.concat()和tf.reshape()函数作用都不是很了解，所以用一个小例子测试了一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">t1 = [[[1, 2], [4, 5]], [[6, 7], [8, 9]]]</span><br><span class="line">seq_output = tf.concat(t1, 1)</span><br><span class="line">x = tf.reshape(seq_output, [-1, 8])</span><br><span class="line">y = tf.reshape(seq_output, [-1, 4])</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(seq_output))</span><br><span class="line">    print(sess.run(x))</span><br><span class="line">    print(sess.run(y))</span><br><span class="line">    </span><br><span class="line"># output</span><br><span class="line">[[1 2 6 7]</span><br><span class="line"> [4 5 8 9]]</span><br><span class="line">[[1 2 6 7 4 5 8 9]]</span><br><span class="line">[[1 2 6 7]</span><br><span class="line"> [4 5 8 9]]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;从输出可以看出，tf.concat()在指定axis为1时将两个子矩阵进行水平拼接，同理指定为0时进行垂直拼接；tf.reshape()完成了output形状的重构，将seq_output重构为shape为[x, lstm_size]的新矩阵，其中x=total_item_num_of_seq_output / lstm_size，要注意必须能够整除，不然会报错。</p>
<p>&emsp;&emsp;然后是初始化权重矩阵w和偏移量b，tf.truncated_normal()表示用截断正态分布对w进行初始化，同样用0初始化偏移量。对于截断正态分布的概念之前也不是很了解，所以用一个简单的例子具体化一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">softmax_w = tf.Variable(tf.truncated_normal([3, 5], mean=10, stddev=0.1))</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    tf.global_variables_initializer().run()</span><br><span class="line">    print(sess.run(softmax_w))</span><br><span class="line">    </span><br><span class="line"># output</span><br><span class="line">[[ 9.999975  9.992491  9.833085  9.996746 10.045398]</span><br><span class="line"> [ 9.882008 10.108604 10.091408  9.901678  9.979222]</span><br><span class="line"> [10.199086 10.119386  9.997254  9.943495  9.905077]]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;其中mean表示生成分布的均值，stddev表示分布的标准差，如果生成的数字偏离均值的两倍标准差以上则丢弃该数字，默认的mean和stddev的默认值分别是0.0和1.0。</p>
<p>&emsp;&emsp;权重矩阵和偏移量确定好初始化方式后，就可以进行逻辑输出logits的构建了，很简单，逻辑输出logits=w*x+b，x是从cell得到的output。计算得到逻辑输出后，需要将输出值从普通的输出格式转换为我们需要的概率格式（0~1的float格式），这个过程需要借助tf.softmax()函数，如果不清楚softmax具体的效果是怎么样的，可以用下面的小例子直观具体的看出来：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">logits = [1.0, 3.0, 6.0]</span><br><span class="line">prediction = tf.nn.softmax(logits, name=&apos;predictions&apos;)</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    print(sess.run(prediction))</span><br><span class="line">    </span><br><span class="line"># output</span><br><span class="line">[0.00637746 0.04712342 0.94649917]</span><br></pre></td></tr></table></figure>
<p>&emsp;&emsp;简而言之，softmax就是讲非概率值转换为0~1范围内的概率值，值越大，对应的输出概率就越大。比如上面例子中0.6对应的输出0.94649917，也就是说它对应的概率值高达94.65%，因为它在三个数字中是最大的，这种概率形式的输出可以方便我们进行后续的预测操作等等。</p>
<p>【未完待续】</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2018/03/05/LSTM-Learning-CharRNN/" data-id="cjedo0ovb000073fuy6lnxy1d" class="article-share-link">Teilen</a>
      
      
    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archiv</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/">March 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">letzter Beitrag</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/03/05/LSTM-Learning-CharRNN/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>